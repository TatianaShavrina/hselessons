{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание 2.\n",
    "## Комсомольская правда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import urllib.request as ur\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "скачать КП - тексты с июля 2015 по декабрь 2016\n",
    "\n",
    "сами тексты отдельными файлами + метаданные в таблице csv табуляцией\n",
    "\n",
    "Метаданные: название статьи, автор, дата, количество слов, путь к файлу\n",
    "\n",
    "сами тексты должны быть распределены по папкам по году и по месяцу в году\n",
    "\n",
    "скачивание с помощью lxml\n",
    "\n",
    "Корпус должен быть очищен и продублирован - plain text и с размееткой  mystem через os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#задаем хэдеры - они понадобятся еще много раз\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/600.3.18 (KHTML, like Gecko) Version/8.0.3 Safari/600.3.18'\n",
    "headers = { 'User-Agent' : user_agent }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#startlink - первая ссылка, с которой мы начнем наш обход ресурса\n",
    "# мне повезло, у КП все тексты имеют в URL айди, соответствующий номеру статьи на ресурсе\n",
    "\n",
    "startlink = \"http://www.kp.ru/online/news/2591074/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# создаем папки с годами - 2015 и 2016, и в каждой по 12 папок с месяцами\n",
    "\n",
    "for year in range(2015,2017):\n",
    "    for month in range(1,13):\n",
    "        if month < 10:\n",
    "            month = \"0\" + str(month)\n",
    "        path = \"/home/mi_air/Downloads/Komsomol_Truth/plain_text/\"+str(year)+\"/\"+str(month)\n",
    "        os.makedirs(path, mode=0o777, exist_ok=False)\n",
    "        path_mystem = \"/home/mi_air/Downloads/Komsomol_Truth/mystem/\"+str(year)+\"/\"+str(month)\n",
    "        os.makedirs(path_mystem, mode=0o777, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# файлы будем называть айдишниками текстов, так как они уникальные\n",
    "# обход начнем с указанного номера \n",
    "num = int(startlink.split(\"/\")[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#напишем функцию, которая будет проверять, находится ли по указанной ссылке текст, в нужном интервале дат:\n",
    "# с июля 2015 по декабрь 2016 (июль 2015 включаем, декабрь 2016 не включаем)\n",
    "# заодно она будет возвращать нам год с месяцем, чтобы удобно было сохранять текст в нужную папку\n",
    "def TextDate(link):\n",
    "    bl = False\n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        date=re.split(\"T\", re.split(\"'pubdate':'\", r.text)[1])[0]\n",
    "        year = int(re.split(\"-\", date)[0])\n",
    "        month = int(re.split(\"-\", date)[1])\n",
    "        if year==2016:\n",
    "            if month<12:\n",
    "                bl=True\n",
    "        elif year==2015:\n",
    "            if month>5:\n",
    "                bl=True\n",
    "        return [bl,year,month]\n",
    "    except:\n",
    "        return [bl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, 2017, 2]\n",
      "[False, 2014, 9]\n",
      "[True, 2015, 8]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "#проверка этой функции\n",
    "print(TextDate(\"http://www.kp.ru/online/news/2644468/\"))\n",
    "print(TextDate(\"http://www.kp.ru/online/news/1844468/\"))\n",
    "print(TextDate(\"http://www.kp.ru/online/news/2144468/\"))\n",
    "print(TextDate(\"http://www.kp.ru/online/news/2644445/\")) #удаленная статья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#напишем функцию, извлекающую текст и метаданные по ссылке\n",
    "def getArticleTextKP(adr):\n",
    "    resp = ur.urlopen(adr)\n",
    "    page = resp.read().decode(\"utf-8\")\n",
    "    resp.close()\n",
    "    tree = lxml.html.fromstring(page)\n",
    "    #со структурой повезло не так сильно: вся основная метаинформация доступна, но лежит текстовым\n",
    "    # словарем внутри dataLayer в теге <script>\n",
    "    \n",
    "    author = re.split(\"',\", re.split(\"'authors\\': \\'\", tree.xpath('.//script[@type=\"text/javascript\"][2]/text()')[0])[1])[0]\n",
    "    date  = re.split(\"T\", re.split(\"'pubdate':'\", tree.xpath('.//script[@type=\"text/javascript\"][2]/text()')[0])[1])[0]\n",
    "    #нужен формат дд.мм.гггг\n",
    "    date2 = date.split(\"-\")\n",
    "    date = \".\".join(date2[::-1])\n",
    "    source = \"Комсомольская Правда\"\n",
    "    title = tree.findtext(\".//title\")\n",
    "    url = adr\n",
    "    \n",
    "    text = re.split(\"</div></article>\",re.split('itemprop=\"articleBody\" id=\"hypercontext\">', page)[1])[0] \n",
    "    #текст доставала так, потому что lxml выкусывал ссылки на именованные сущности внутри текста. зато почистила потом \n",
    "    # режимом lxml в BeautifulSoup:\n",
    "    beaux_text=BeautifulSoup(text, \"lxml\")\n",
    "    text=beaux_text.get_text() \n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    wordcount = str(len(text.split(\" \")))\n",
    "    \n",
    "    #time.sleep(random.uniform(0, 2)) #чтобы нас не забанили\n",
    "\n",
    "    return [author, date, source, title, url, wordcount, text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metatable = open(r'/home/mi_air/Downloads/Komsomol_Truth/metatable.csv', \"w\", encoding=\"utf-8\")\n",
    "firstline = 'path;author;date;source;title;url;wordcount' \n",
    "metatable.write(firstline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#при проверке функции TextDate мы выяснили, что в районе 1,8 млн статей нужные нам статьи уже заканчиваются, \n",
    "#поэтому не будет заставлять программу перебирать номера статей так долго и ограничим их этим числом\n",
    "\n",
    "while num>1866621:\n",
    "    link = \"http://www.kp.ru/online/news/\"+str(num)+\"/\"\n",
    "    \n",
    "    if TextDate(link)[0]==True:\n",
    "        \n",
    "        month,year = TextDate(link)[2],TextDate(link)[1]\n",
    "        if month<10:\n",
    "            month = \"0\"+str(month)\n",
    "        \n",
    "        try:\n",
    "            #author;date;source;title;url;wordcount\n",
    "            params = getArticleTextKP(link)\n",
    "            metaline = \";\".join(params[:6])\n",
    "            text = params[6]\n",
    "            filename = '/home/mi_air/Downloads/Komsomol_Truth/plain_text/' + str(year) + \"/\" + str(month)+\"/\" + str(num) + \".txt\"\n",
    "            metaline = filename + \";\" +  metaline \n",
    "            metatable.write(metaline + \"\\n\")\n",
    "            textfile = open(filename, \"w\", encoding=\"utf-8\")\n",
    "            for line in text:\n",
    "                textfile.write(line)\n",
    "            #print(filename)\n",
    "        except:\n",
    "            pass\n",
    "    num-=17 #в нескольких запусках шаг также был 100, 3 и 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологическая разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# теперь обойдем все папки с plain text и вызовем mystem для файлов в них\n",
    "for top, dirs, files in os.walk('/home/mi_air/Downloads/Komsomol_Truth/plain_text/'):\n",
    "    for file in files:\n",
    "        filename = os.path.join(top, file)\n",
    "        outfilename = re.sub(\"plain_text\", \"mystem\", filename)\n",
    "        os.system('/home/mi_air/mystem -ni -d ' +filename + \" \" + outfilename)\n",
    "        # print(outfilename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hittite language transliteration Shavrina\n",
    "\n",
    "На сервере в home/guest1/2017/Shavrina/hettencode.py\n",
    "\n",
    "Использование: python hettencode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import getopt\n",
    "import argparse\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_html(syllab_file):\n",
    "    file = open(syllab_file, \"r\", encoding=\"utf8\").read()\n",
    "    syllabtext = []\n",
    "    htmlhead = re.split('<body lang=\"ru-RU\" dir=\"ltr\">', file)[0]\n",
    "    htmlbody = re.split('</p>', re.split(\"<div id=\", file)[0])\n",
    "    for i in range (len(htmlbody)):\n",
    "        htmlbody[i] = re.sub(\"</span></font></sup>\",\"<separator>\",htmlbody[i])\n",
    "        htmlbody[i] = re.sub(\"</span></font></sub>\",\"<separator>\",htmlbody[i])\n",
    "        htmlbody[i] = re.sub(\"</span></font>\",\"<separator>\",htmlbody[i])\n",
    "        lst = re.split(\"<separator>\",htmlbody[i])\n",
    "        for y in lst:\n",
    "            if \"remaining\" not in y or \"missing\" not in y :\n",
    "                syllabtext.append(y)\n",
    "    htmlbottom = \"</p>\\n</body>\\n</html>\" #отрезаем комментарии\n",
    "    for i in range(len(syllabtext)):\n",
    "        if '<a class=' in syllabtext[i]:\n",
    "            syllabtext[i] = \"\".join([re.split(\"<a class=\",syllabtext[i])[0], \"'\"]) #отрезаем сноски\n",
    "\n",
    "    return syllabtext,htmlhead,htmlbottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vowels = ['aeiuAEIU']\n",
    "\n",
    "def token_lang(token,cleantoken):\n",
    "    #определяем язык слова \n",
    "    \n",
    "    if \"<i>\" in token:\n",
    "        lang = 'akkad'\n",
    "    elif cleantoken.isupper():\n",
    "        lang = 'shumer'\n",
    "    else:\n",
    "        lang = 'hett' #default\n",
    "    return (lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def script(token):\n",
    "    #определяем написание\n",
    "    script_type = 'normal'\n",
    "    if '<sup>' in token:\n",
    "        script_type = 'superscript'\n",
    "    elif '<sub>' in token:\n",
    "        script_type = 'subscript'     \n",
    "    return(script_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_token(token): #только само хеттское слово, без тегов\n",
    "    fin = re.sub(\"'\",\"\", token)\n",
    "    fin = re.sub('</i>',\"\", fin)\n",
    "    cleantoken1 = re.split(\">\",fin)\n",
    "    cleantoken = cleantoken1[-1]\n",
    "    return(cleantoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def html_feat(token):\n",
    "    try:\n",
    "        fontsize = re.split('\"', re.split('<font size=\"', token)[1])[0]\n",
    "        style = re.split('\"', re.split('style=\"', token)[1])[0]\n",
    "        spanlang = re.split('\"', re.split('<span lang=\"', token)[1])[0]\n",
    "        return(fontsize, spanlang, style)\n",
    "    except:\n",
    "        fontsize = 2\n",
    "        spanlang = \"it-IT\"\n",
    "        style = \"font-size: 9pt\"\n",
    "        return(fontsize, spanlang, style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lonely_vowels(i,cleantoken):\n",
    "    if i==0 and len(cleantoken)>1 and cleantoken[i+1]==\"-\":\n",
    "        features=\"lonely,start\"\n",
    "    elif i==(len(cleantoken)-1) and cleantoken[i-1]==\"-\":\n",
    "        features=\"lonely,end\"\n",
    "    elif cleantoken[i-1]==\"-\" and cleantoken[i+1]==\"-\":\n",
    "        features=\"lonely,mid\"\n",
    "    else:\n",
    "        features=\"notlonely\"\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def operations(token_dict):\n",
    "    j=0\n",
    "    new_dict = {}\n",
    "    garbage = ['*', '~','!', '?', '[', ']', '˹', '˺', '˼', '˻']\n",
    "    for i in range(len(token_dict)):\n",
    "        if token_dict[i][0]==\"-\":\n",
    "            if  token_dict[i-1][1]==token_dict[i+1][1]:\n",
    "                new_dict[j] = token_dict[i]\n",
    "                j+=1\n",
    "            else:\n",
    "                token_dict[i][0]=re.sub('-','',token_dict[i][0])\n",
    "                new_dict[j] = token_dict[i]\n",
    "                j+=1\n",
    "\n",
    "        elif token_dict[i][1]=='hett':\n",
    "            if token_dict[i][0]=='í':\n",
    "                token_dict[i][0]=re.sub('í','i',token_dict[i][0])\n",
    "                new_dict[j] = token_dict[i]\n",
    "                j+=1\n",
    "            elif token_dict[i][2]=='subscript':\n",
    "                pass\n",
    "\n",
    "        elif token_dict[i][0]=='i':\n",
    "            if  token_dict[i-1][0] in vowels :\n",
    "                if token_dict[i+1][0] in vowels:\n",
    "                    token_dict[i][0]=re.sub('i','y',token_dict[i][0])\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "\n",
    "        elif token_dict[i][0] in vowels :\n",
    "            if token_dict[i+1][0]==token_dict[i][0]:\n",
    "                if token_dict[i][4].startswith('lonely') or token_dict[i+1][4].startswith('lonely'):\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "                else:\n",
    "                    token_dict[i][0]=\"\" #смотрим только вперед, остается только вторая гласная\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "                \n",
    "\n",
    "        elif token_dict[i][0].isalpha()==False:\n",
    "            pass\n",
    "        \n",
    "        elif token_dict[i][0] in garbage:\n",
    "            pass\n",
    "\n",
    "        elif token_dict[i][0] in vowels :\n",
    "            if token_dict[i+1][0]==token_dict[i][0]:\n",
    "                if i==0 or i==len((token_dict)-1):\n",
    "                    token_dict[i][0] = re.sub('a','ā',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('u','ū',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('e','ē',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('i','ī',token_dict[i][0])\n",
    "                    token_dict[i+1][0] = re.sub(token_dict[i+1][0],'',token_dict[i+1][0])\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "                elif token_dict[i][4].startswith('lonely') | token_dict[i+1][4].startswith('lonely'):\n",
    "                    token_dict[i][0] = re.sub('a','ā',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('u','ū',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('e','ē',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('i','ī',token_dict[i][0])\n",
    "                    token_dict[i+1][0] = re.sub(token_dict[i+1][0],'',token_dict[i+1][0])\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "                elif token_dict[i+2][0]==token_dict[i][0]:\n",
    "                    token_dict[i][0] = re.sub('a','ā',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('u','ū',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('e','ē',token_dict[i][0])\n",
    "                    token_dict[i][0] = re.sub('i','ī',token_dict[i][0])\n",
    "                    token_dict[i+1][0] = re.sub(token_dict[i+1][0],'',token_dict[i+1][0])\n",
    "                    token_dict[i+2][0] = re.sub(token_dict[i+2][0],'',token_dict[i+2][0])\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "                else:\n",
    "                    new_dict[j] = token_dict[i]\n",
    "                    j+=1\n",
    "\n",
    "        elif token_dict[i][0]=='h':\n",
    "            token_dict[i][0]=re.sub('h','ḫ',token_dict[i][0])\n",
    "            new_dict[j] = token_dict[i]\n",
    "            j+=1\n",
    "\n",
    "        elif token_dict[i][0]=='s':\n",
    "            token_dict[i][0]=re.sub('s','š',token_dict[i][0])\n",
    "            new_dict[j] = token_dict[i]\n",
    "            j+=1\n",
    "\n",
    "        else:\n",
    "            new_dict[j] = token_dict[i]\n",
    "            j+=1\n",
    "\n",
    "        return(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_token(token,lang,script_type,html):\n",
    "    # сначала разные языки букв\n",
    "    restored_token = token\n",
    "    if lang =='akkad':\n",
    "        \n",
    "        restored_token= '<font size=\"' + str(html[0]) + ' style=' + html[2] + '\"><span lang=\"' + html[1] + '\">' + \"<i>\" + restored_token + \"</i>\" + '</span></font>'\n",
    "    else:\n",
    "        restored_token = '<font size=\"' + str(html[0]) + ' style=' + html[2] + '\"><span lang=\"' + html[1] + '\">' + restored_token + '</span></font>'\n",
    "            \n",
    "    if script_type == '<sup>':\n",
    "        restored_token=\"<sup>\" + restored_token + \"</sup>\"\n",
    "    elif script_type == '<sub>':\n",
    "        restored_token=\"<sub>\" + restored_token + \"</sub>\"\n",
    "    return(restored_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def syll_to_short(syllabtext):\n",
    "    shorttext = []\n",
    "\n",
    "    #сделаем из каждого токена словарь, ключом которого будет порядковый номер буквы,\n",
    "    # а значениями - сама буква и ее признаки: язык, одиноч и ее позиция, суперскрипт/субскрипт, html\n",
    "    for token in syllabtext:\n",
    "\n",
    "        token_dict = {}\n",
    "        \n",
    "        cleantoken = clean_token(token)\n",
    "        if len(cleantoken)>0:\n",
    "\n",
    "            for j in range(len(cleantoken)):\n",
    "                if cleantoken[j] in vowels:\n",
    "                    features = lonely_vowels(j,cleantoken)\n",
    "                else:\n",
    "                    features = \"cons\" \n",
    "                token_dict[j] = [cleantoken[j], script(token), token_lang(token,cleantoken),  html_feat(token), features]\n",
    "\n",
    "            new_dic = operations(token_dict)\n",
    "            print(new_dic)\n",
    "            word = \"\"\n",
    "            for i in range(len(new_dic)):\n",
    "                word += new_dic[i][0]\n",
    "\n",
    "                lang, script1, html = new_dic[i][2], new_dic[i][1], new_dic[i][3]\n",
    "\n",
    "            new = restore_token(word, lang, script1, html)\n",
    "            word = \"\"\n",
    "            shorttext.append(new)\n",
    "    return (shorttext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_html(syllab_file, short_file):\n",
    "    syllabtext,htmlhead,htmlbottom = read_html(syllab_file)\n",
    "\n",
    "    htmlhead += '<body lang=\"ru-RU\" dir=\"ltr\">\\n'\n",
    "    shorttext = syll_to_short(syllabtext)\n",
    "    html = htmlhead\n",
    "    for i in range(len(shorttext)):\n",
    "        html += shorttext[i]\n",
    "    html += htmlbottom\n",
    "    outfile = open(short_file, \"w\", encoding=\"utf8\")\n",
    "    outfile.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'lang' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-955aca779f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msyllab_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\USER\\Documents\\hse\\progr\\syllab1.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mshort_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\USER\\Documents\\hse\\progr\\syllab1_short.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcreate_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyllab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b579dec2f724>\u001b[0m in \u001b[0;36mcreate_html\u001b[0;34m(syllab_file, short_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhtmlhead\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'<body lang=\"ru-RU\" dir=\"ltr\">\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mshorttext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msyll_to_short\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyllabtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtmlhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshorttext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-167caa7d7dd6>\u001b[0m in \u001b[0;36msyll_to_short\u001b[0;34m(syllabtext)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mshorttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'lang' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #args = parser.parse_args()\n",
    "    #if len(args) != 2:\n",
    "        #sys.exit(\"Использование: python hettencode.py syllab_file short_file\")\n",
    "    #syllab_file, short_file = args\n",
    "    syllab_file = r\"C:\\Users\\USER\\Documents\\hse\\progr\\syllab1.html\"\n",
    "    short_file = r\"C:\\Users\\USER\\Documents\\hse\\progr\\syllab1_short.html\"\n",
    "    create_html(syllab_file, short_file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
